{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptrons - MLP\n",
    "\n",
    "This notebook will explore and implement a MLP based neural network for next character pedictions based on the paper: [A Neural Probablisitc Language Model by Bengio, et al.](https://dl.acm.org/doi/pdf/10.5555/944919.944966)\n",
    "\n",
    "In this paper the authors attempt to address the \"curse of dimensionality\" (a word\n",
    "sequence on which the model will be tested is likely to be different from all the word sequences seen\n",
    "during training)\n",
    "\n",
    "The paper aims to mitigate this by \"learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences.\"\n",
    "\n",
    "## Probability of Next Word\n",
    "$$\\hat{P}(w_1^T) = \\prod_{t=1}^{T} \\hat{P}(w_t | w_1^{t-1})$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Diagram\n",
    "![MLP Model](img/mlp_diagram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "SEED = 2697\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in all the words\n",
    "words = open(\"../data/names.txt\", 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'a',\n",
       " 2: 'b',\n",
       " 3: 'c',\n",
       " 4: 'd',\n",
       " 5: 'e',\n",
       " 6: 'f',\n",
       " 7: 'g',\n",
       " 8: 'h',\n",
       " 9: 'i',\n",
       " 10: 'j',\n",
       " 11: 'k',\n",
       " 12: 'l',\n",
       " 13: 'm',\n",
       " 14: 'n',\n",
       " 15: 'o',\n",
       " 16: 'p',\n",
       " 17: 'q',\n",
       " 18: 'r',\n",
       " 19: 's',\n",
       " 20: 't',\n",
       " 21: 'u',\n",
       " 22: 'v',\n",
       " 23: 'w',\n",
       " 24: 'x',\n",
       " 25: 'y',\n",
       " 26: 'z',\n",
       " 0: '.'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the vocabulary of characters and mappings to/from integers\n",
    "\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)} \n",
    "stoi['.'] = 0 # Create the special start of word/end of word token and assign it to label 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "... ---> e\n",
      "..e ---> m\n",
      ".em ---> m\n",
      "emm ---> a\n",
      "mma ---> .\n",
      "olivia\n",
      "... ---> o\n",
      "..o ---> l\n",
      ".ol ---> i\n",
      "oli ---> v\n",
      "liv ---> i\n",
      "ivi ---> a\n",
      "via ---> .\n",
      "ava\n",
      "... ---> a\n",
      "..a ---> v\n",
      ".av ---> a\n",
      "ava ---> .\n",
      "isabella\n",
      "... ---> i\n",
      "..i ---> s\n",
      ".is ---> a\n",
      "isa ---> b\n",
      "sab ---> e\n",
      "abe ---> l\n",
      "bel ---> l\n",
      "ell ---> a\n",
      "lla ---> .\n",
      "sophia\n",
      "... ---> s\n",
      "..s ---> o\n",
      ".so ---> p\n",
      "sop ---> h\n",
      "oph ---> i\n",
      "phi ---> a\n",
      "hia ---> .\n"
     ]
    }
   ],
   "source": [
    "# Build the dataset\n",
    "\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "X, Y = [], []\n",
    "for w in words[:5]:\n",
    "    print(w)\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "        context = context[1:] + [ix] # crop and append\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.int64, torch.Size([32]), torch.int64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Context\n",
    "\n",
    "If we create a dataset X which is tracking words (or characters in this case) two steps before the current word and a dataset Y which contains the following word we will be able to create a neural network that will have weights tuned to have some numerical model of these previous word relationships\n",
    "\n",
    "This is what builds \"context\"!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([27, 2]), torch.Size([32, 3]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's build a look up table C - embedding tables\n",
    "torch.manual_seed(SEED)\n",
    "C = torch.randn((27,2))\n",
    "\n",
    "C.shape, X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embed X in C ... This is like a forward pass?\n",
    "emb = C[X] # <-- This is so cool python list comprehension. In this case we are mapping\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = torch.randn((6,100))\n",
    "b1 = torch.randn(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch tensor magic - using the cat function we can reshape a tensor.\n",
    "In this case we have out embedding matrix. This matrix is similar to a linear layer. We are mapping out index tokens for the inputs (the context list) to two randomly initialiazed look-up table. Since the embedding matrix has a size of [32,3,2] and our weights layer is [6,100] we cannot perform matrix multiplication. Here we see the function `torch.cat([list of tensors to concatenate], dimension you wish to cat along)`. \n",
    "\n",
    "The first example is not ideal because it is limited to a context window of 3. If we use a function called `torch.unbind(input, dim=0)` our tensor `emb` will be cated together for as many context variables there are (1 being the dimension where these variables are stored). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 6]), torch.Size([32, 6]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([emb[:,0, :], emb[:, 1, :], emb[:, 2, :]], 1).shape, torch.cat(torch.unbind(emb, 1), 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There is an even more efficient method using view\n",
    "a = torch.arange(18)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1],\n",
       "         [ 2,  3],\n",
       "         [ 4,  5]],\n",
       "\n",
       "        [[ 6,  7],\n",
       "         [ 8,  9],\n",
       "         [10, 11]],\n",
       "\n",
       "        [[12, 13],\n",
       "         [14, 15],\n",
       "         [16, 17]]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view(3,3,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " the tensor object is stored strictly as a one dimensional array in memory. When we call `torch.view` we are able to choose how we are viewing the data in memory. So we can reshape any tensor array as long as the shape is made of multiples of len(tensor) and equals that length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hooray\n"
     ]
    }
   ],
   "source": [
    "if emb.view([32,6]).shape == torch.cat(torch.unbind(emb,1),1).shape:\n",
    "    print(\"hooray\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now remember we have to account for possible differences in how many values ar in X. For the exmple we used 32, but the amount of input values may vary.\n",
    "# Using -1 -> pytorch will infer what it should be\n",
    "# Let's pass through the net!\n",
    "h = emb.view(-1,6) @ W1 +b1\n",
    "\n",
    "# Don't forget tanh activation function!\n",
    "h = torch.tanh(h)\n",
    "h.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the final layer!\n",
    "torch.manual_seed(SEED)\n",
    "W2 = torch.randn((100,27))\n",
    "b2 = torch.randn(27)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logits\n",
    "Now we pass h (hidden) to the next layer where we will get the logits of the network\n",
    "\n",
    "What are logits - this is the raw, unnormalized( not between 0-1) output values. These are used particularly when you are applying a softmax function which will take the logits along a particular dimension and normalize them to be from values of 0-1 and sum to 1.\n",
    "$$logit(p) = log(\\frac{p}{1-p})$$\n",
    "\n",
    "$$softmax(z)_i = \\frac{e^{z_i}}{\\Sigma_{j=1}^{K}e^{z_j}}$$\n",
    "\n",
    "Next we have log counts. This refers to the logarithm of some count or frequency of data - this is useful when data spans over severl orders of magnitude. (text mining, bioinformatics)\n",
    "\n",
    "Logarithmic transformation ca help in these cased by reducing the range and dampening th effect of outliers or extreme values.\n",
    "\n",
    "It is important to not when taking the log of some count adding 1 will help account for count = 0 where log(0) is undefined.\n",
    "\n",
    "For us we already have the log counts (logits) and in order to move back to get the counts we simply exponentiate our value to undo the logarithmic transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the logits\n",
    "logits = h @ W2 +b2\n",
    "counts = logits.exp()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
